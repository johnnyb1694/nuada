import logging
import json
import pandas as pd
import nltk

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

log = logging.getLogger()
log.setLevel('INFO')

def _download_nltk_data(download_dir: str = '/tmp'):
    """
    Helper function to download pre-requisite tokenizer artifacts for tokenization purposes.
    """
    try:
        nltk.data.path.append(download_dir)
        nltk.download('punkt', quiet=True, download_dir=download_dir)
        nltk.download('stopwords', quiet=True, download_dir=download_dir)
    except IOError as e:
        log.error(e)
        return False
    return True

def _deserialise(response) -> dict:
    """
    Ingests a response from `boto3.client('s3').get_object()` and deserialises this response into a Python dictionary object.

    :param response: Raw response from call to `boto3.client('s3').get_object()` (see `lambda_handler()` presented below)
    """
    body = response['Body'].read().decode('utf-8')
    decoded = json.loads(body)
    return decoded

def _extract_headlines(decoded: dict) -> pd.DataFrame:
    """
    Processes the article data output generated by `deseralise_article_data()`.

    :param article_data: Output of `deserialise_article_data()`
    """
    articles = decoded['response']['docs']
    headlines = [{'publication_date': pd.to_datetime(article['pub_date']),
                  'headline': article['headline']['main']} for article in articles]
    headlines_df = pd.DataFrame(headlines)
    headlines_df['year'] = headlines_df['publication_date'].dt.year
    headlines_df['month'] = headlines_df['publication_date'].dt.month
    return headlines_df

def _tokenize_headlines(headlines_df: pd.DataFrame):
    """
    Ingests dataframe of headlines and expands each 'term' contained within the headline into a separate row.

    :param headlines_df: `pd.DataFrame` object with *at least* column `headline`
    """
    headlines_df['term'] = headlines_df['headline'].apply(word_tokenize)
    terms_df = headlines_df.explode('term')
    terms_df['term'] = terms_df['term'].str.lower()

    stop_words = set(stopwords.words('english')) # NB: set-based usage improves lookup efficiency over list-based usage
    terms_df = terms_df[~terms_df['term'].isin(stop_words)]
    terms_df = terms_df[terms_df['term'].apply(lambda xx: xx.isalpha())]

    return terms_df

def _aggregate_terms(terms_df: pd.DataFrame, grain: list[str] = ['term', 'year', 'month']) -> pd.DataFrame:
    """
    Aggregate headline terms by the list of strings specified in `by`

    :param terms_df: `pd.DataFrame` object with *at least* columns `term`, `year` and `month`
    """
    aggregation = terms_df.groupby(by=grain).size().reset_index(name='frequency')
    return aggregation

def preprocess(response: dict) -> pd.DataFrame:
    """
    Preprocesses the response delivered by a call to `boto3.client('s3').get_object()`. In particular, this response is a 
    dictionary object with two keys `Body` and `Metadata` (the former is subsequently accessed by the internal `_deseralise()` function)

    :param response: Raw response from call to `boto3.client('s3').get_object()`
    """
    _download_nltk_data()
    decoded = _deserialise(response)
    terms_df = (_extract_headlines(decoded)
                    .pipe(_tokenize_headlines)
                    .pipe(_aggregate_terms))

    return terms_df

if __name__ == '__main__':

    import boto3

    s3_client = boto3.client('s3')
    response = s3_client.get_object(Bucket='nuada-archives', Key='2023_11_nyt_archive_search.json')

    res = preprocess(response)
    print(res)