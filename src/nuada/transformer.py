import logging
import json
import pandas as pd
import nltk

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

log = logging.getLogger()
log.setLevel('INFO')

def _download_nltk_data(download_dir: str = '/tmp'):
    """
    Helper function to download pre-requisite tokenizer artifacts for tokenization purposes.
    """
    try:
        nltk.data.path.append(download_dir)
        nltk.download('punkt', quiet=True, download_dir=download_dir)
        nltk.download('stopwords', quiet=True, download_dir=download_dir)
    except IOError as e:
        log.error(e)
        return False
    return True

def _deserialise(response) -> dict:
    """
    Ingests a response from `boto3.client('s3').get_object()` and deserialises this response into a Python dictionary object.

    :param response: Raw response from call to `boto3.client('s3').get_object()` (see `lambda_handler()` presented below)
    """
    body = response['Body'].read().decode('utf-8')
    decoded = json.loads(body)
    return decoded

def _extract_headlines(decoded: dict) -> pd.DataFrame:
    """
    Processes the article data output generated by `deseralise_article_data()`.

    :param article_data: Output of `deserialise_article_data()`
    """
    articles = decoded['response']['docs']
    headlines = [{'publication_date': pd.to_datetime(article['pub_date']),
                  'headline': article['headline']['main']} for article in articles]
    headlines_df = pd.DataFrame(headlines)
    headlines_df['publication_year'] = headlines_df['publication_date'].dt.year
    headlines_df['publication_month'] = headlines_df['publication_date'].dt.month
    return headlines_df

def _tokenize_headlines(headlines_df: pd.DataFrame):
    """
    Ingests dataframe of headlines and expands each 'term' contained within the headline into a separate row.

    :param headlines_df: `pd.DataFrame` object with *at least* column `headline`
    """
    headlines_df['headline_term'] = headlines_df['headline'].apply(word_tokenize)
    headlines_df = headlines_df.explode('headline_term')
    headlines_df['headline_term'] = headlines_df['headline_term'].str.lower()

    stop_words = set(stopwords.words('english')) # NB: set-based usage improves lookup efficiency over list-based usage
    headlines_df = headlines_df[~headlines_df['headline_term'].isin(stop_words)]
    headlines_df = headlines_df[headlines_df['headline_term'].apply(lambda xx: xx.isalpha())]

    return headlines_df

def _aggregate_headlines(headlines_df: pd.DataFrame, grain: list[str] = ['publication_year', 'publication_month', 'headline_term']) -> pd.DataFrame:
    """
    Aggregate headline terms by the list of strings specified in `by`

    :param headlines_df: `pd.DataFrame` object with *at least* columns `headline_term`, `publication_year` and `publication_month`
    """
    aggregation = headlines_df.groupby(by=grain).size().reset_index(name='count')
    return aggregation

def preprocess(response: dict) -> pd.DataFrame:
    """
    Preprocesses the response delivered by a call to `boto3.client('s3').get_object()`. In particular, this response is a 
    dictionary object with two keys `Body` and `Metadata` (the former is subsequently accessed by the internal `_deseralise()` function)

    :param response: Raw response from call to `boto3.client('s3').get_object()`
    """
    _download_nltk_data()
    decoded = _deserialise(response)
    headlines_df = _extract_headlines(decoded)
    tokenized = _tokenize_headlines(headlines_df)
    aggregated = _aggregate_headlines(tokenized)
    return aggregated

if __name__ == '__main__':

    import boto3

    s3_client = boto3.client('s3')
    response = s3_client.get_object(Bucket='nuada-archives', Key='2023_11_nyt_archive_search.json')

    res = preprocess(response)
    print(res)